{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f846ea7",
   "metadata": {},
   "source": [
    "# 階層型クラスタリング（平均スペクトル）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- ユーザー設定 ---\n",
    "main_dir = Path(\"C:/Users/sawamoto24/sawamoto24/master/microplastic/data\")\n",
    "dataset1_folder_name = \"MPs_20250911\"\n",
    "dataset2_folder_name = \"MPs_20250905_2\"\n",
    "csv_filename = \"pixel_features_plastics_only.csv\"\n",
    "# --------------------\n",
    "\n",
    "# --- ステップ1: 平均スペクトルデータの作成 ---\n",
    "print(\"--- ステップ1: 平均スペクトルデータの作成 ---\")\n",
    "\n",
    "# 1. 2つのデータセットを読み込み、結合する\n",
    "try:\n",
    "    df1 = pd.read_csv(main_dir / dataset1_folder_name / \"csv\" / csv_filename)\n",
    "    df2 = pd.read_csv(main_dir / dataset2_folder_name / \"csv\" / csv_filename)\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    print(\"データセットの結合が完了しました。\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"エラー: CSVファイルが見つかりません: {e.filename}\")\n",
    "    exit()\n",
    "\n",
    "# 2. 特徴量とラベルに分割\n",
    "X_full = combined_df.drop(columns=['label_name', 'original_index'])\n",
    "y_full = combined_df['label_name']\n",
    "\n",
    "# 3. 各プラスチックの平均スペクトルを計算\n",
    "mean_spectra_df = pd.concat([X_full, y_full], axis=1).groupby('label_name').mean()\n",
    "print(\"各プラスチックの平均スペクトルを計算しました。\")\n",
    "\n",
    "# --- ステップ2: コサイン類似度行列の作成 ---\n",
    "print(\"\\n--- ステップ2: コサイン類似度行列の作成 ---\")\n",
    "\n",
    "# コサイン類似度を計算 (値が1に近いほど似ている)\n",
    "similarity_matrix = cosine_similarity(mean_spectra_df)\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=mean_spectra_df.index, columns=mean_spectra_df.index)\n",
    "print(\"コサイン類似度行列:\")\n",
    "print(similarity_df)\n",
    "\n",
    "# --- ステップ3: 階層的クラスタリングの実行 ---\n",
    "print(\"\\n--- ステップ3: 階層的クラスタリングの実行 ---\")\n",
    "\n",
    "# 類似度(similarity)を距離(distance)に変換\n",
    "distance_matrix = 1 - similarity_matrix\n",
    "\n",
    "# 浮動小数点誤差を補正するため、対角成分を強制的に0にする\n",
    "np.fill_diagonal(distance_matrix, 0)\n",
    "\n",
    "condensed_distance = squareform(distance_matrix)\n",
    "\n",
    "# 階層的クラスタリングを実行\n",
    "linked = hierarchy.linkage(condensed_distance, method='ward')\n",
    "print(\"階層的クラスタリングが完了しました。\")\n",
    "\n",
    "\n",
    "# --- ステップ4: デンドログラムの可視化 ---\n",
    "print(\"\\n--- ステップ4: デンドログラムの可視化 ---\")\n",
    "# ★★★ スタイルシート名を新しいバージョン対応のものに変更 ★★★\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# デンドログラムを描画\n",
    "hierarchy.dendrogram(\n",
    "    linked,\n",
    "    orientation='top',\n",
    "    labels=mean_spectra_df.index,\n",
    "    distance_sort='descending',\n",
    "    show_leaf_counts=True\n",
    ")\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram of Plastics', fontsize=16)\n",
    "plt.ylabel('Distance (1 - Cosine Similarity)', fontsize=12)\n",
    "plt.xlabel('Plastic Type', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a53aa",
   "metadata": {},
   "source": [
    "種類別生産割合の追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b02b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- ユーザー設定 ---\n",
    "main_dir = Path(\"C:/Users/sawamoto24/sawamoto24/master/microplastic/data\")\n",
    "dataset1_folder_name = \"MPs_20250911\"\n",
    "dataset2_folder_name = \"MPs_20250905_2\"\n",
    "csv_filename = \"pixel_features_plastics_only.csv\"\n",
    "# --------------------\n",
    "\n",
    "# --- ステップ1: 平均スペクトルデータの作成 ---\n",
    "print(\"--- ステップ1: 平均スペクトルデータの作成 ---\")\n",
    "\n",
    "# 1. 2つのデータセットを読み込み、結合する\n",
    "try:\n",
    "    df1 = pd.read_csv(main_dir / dataset1_folder_name / \"csv\" / csv_filename)\n",
    "    df2 = pd.read_csv(main_dir / dataset2_folder_name / \"csv\" / csv_filename)\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    print(\"データセットの結合が完了しました。\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"エラー: CSVファイルが見つかりません: {e.filename}\")\n",
    "    exit()\n",
    "\n",
    "# 2. 特徴量とラベルに分割\n",
    "X_full = combined_df.drop(columns=['label_name', 'original_index'])\n",
    "y_full = combined_df['label_name']\n",
    "\n",
    "# 3. 各プラスチックの平均スペクトルを計算\n",
    "mean_spectra_df = pd.concat([X_full, y_full], axis=1).groupby('label_name').mean()\n",
    "print(\"各プラスチックの平均スペクトルを計算しました。\")\n",
    "\n",
    "# --- ステップ2: コサイン類似度行列の作成 ---\n",
    "print(\"\\n--- ステップ2: コサイン類似度行列の作成 ---\")\n",
    "similarity_matrix = cosine_similarity(mean_spectra_df)\n",
    "\n",
    "# --- ステップ3: 階層的クラスタリングの実行 ---\n",
    "print(\"\\n--- ステップ3: 階層的クラスタリングの実行 ---\")\n",
    "distance_matrix = 1 - similarity_matrix\n",
    "np.fill_diagonal(distance_matrix, 0)\n",
    "condensed_distance = squareform(distance_matrix)\n",
    "linked = hierarchy.linkage(condensed_distance, method='ward')\n",
    "print(\"階層的クラスタリングが完了しました。\")\n",
    "\n",
    "# --- ステップ4: デンドログラムの可視化 ---\n",
    "print(\"\\n--- ステップ4: デンドログラムの可視化 ---\")\n",
    "\n",
    "# ★★★ ここからが変更点 ★★★\n",
    "# 1. 生産割合のデータを定義 (ご提示のグラフより)\n",
    "production_ratios = {\n",
    "    'PP': 19.0,\n",
    "    'LDPE': 14.0, # グラフでは LD-PE\n",
    "    'PVC': 12.8,\n",
    "    'HDPE': 12.2, # グラフでは HD-PE\n",
    "    'PET': 6.2,\n",
    "    'PS': 5.2,\n",
    "    'PC': 1.9,   # PC, PMMA, ABSはグラフにないため不明\n",
    "    'PMMA': 0.8,\n",
    "    'ABS': 2.4\n",
    "}\n",
    "\n",
    "# 2. デンドログラムのラベルを「名前 + 生産割合」の形式に動的に作成\n",
    "original_labels = mean_spectra_df.index\n",
    "new_labels = []\n",
    "for label in original_labels:\n",
    "    ratio = production_ratios.get(label)\n",
    "    if pd.notna(ratio):\n",
    "        new_labels.append(f\"{label} ({ratio}%)\")\n",
    "    else:\n",
    "        new_labels.append(f\"{label} (N/A)\")\n",
    "# ★★★ ここまでが変更点 ★★★\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# デンドログラムを描画 (★★★ labelsに新しいラベルリストを使用 ★★★)\n",
    "hierarchy.dendrogram(\n",
    "    linked,\n",
    "    orientation='top',\n",
    "    labels=new_labels,\n",
    "    distance_sort='descending',\n",
    "    show_leaf_counts=True\n",
    ")\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram of Plastics (with Production Ratios)', fontsize=16)\n",
    "plt.ylabel('Distance (1 - Cosine Similarity)', fontsize=12)\n",
    "plt.xlabel('Plastic Type (Production Ratio %)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f7877c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3582123",
   "metadata": {},
   "source": [
    "## インスタンス（プラスチック片）単位"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a3d79c",
   "metadata": {},
   "source": [
    "### Baseline：通常の9クラス分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d83900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import labelme\n",
    "import json\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import re\n",
    "from skimage import measure # 連結成分ラベリングに使用\n",
    "\n",
    "# --- ユーザー設定 ---\n",
    "# このスクリプトは、2つのデータセットフォルダそれぞれに対して実行する必要があります\n",
    "folder_name = \"MPs_20250911\" # まずはこちらで実行\n",
    "# folder_name = \"MPs_20250905_2\" # 次にこちらで実行\n",
    "main_dir = Path(f\"C:/Users/sawamoto24/sawamoto24/master/microplastic/data/{folder_name}\")\n",
    "reference_file_stem = f\"{folder_name}_Ex-1_Em-1_ET300_step1\"\n",
    "# --------------------\n",
    "\n",
    "\n",
    "# --- ステップ1: インスタンスマスクの作成 ---\n",
    "print(f\"--- {folder_name}: インスタンスID付きデータセットの作成 ---\")\n",
    "json_path = main_dir / (reference_file_stem + \".json\")\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "image_size = (data['imageHeight'], data['imageWidth'])\n",
    "labels_in_json = sorted(list(set(shape['label'] for shape in data['shapes'])))\n",
    "label_name_to_value = {label: i for i, label in enumerate(labels_in_json, start=1)}\n",
    "\n",
    "class_label_mask, _ = labelme.utils.shapes_to_label(image_size, data['shapes'], label_name_to_value)\n",
    "\n",
    "# 連結成分ラベリングで各プラスチック片にユニークIDを割り振る\n",
    "instance_mask = np.zeros_like(class_label_mask, dtype=int)\n",
    "instance_id_counter = 1\n",
    "for label_name, label_value in label_name_to_value.items():\n",
    "    if label_name not in ['other', 'background', 'background_ref']:\n",
    "        binary_mask = (class_label_mask == label_value)\n",
    "        # 各連結成分（個々の物体）にIDを振る\n",
    "        labeled_components, num_components = measure.label(binary_mask, connectivity=2, return_num=True)\n",
    "        for i in range(1, num_components + 1):\n",
    "            instance_mask[labeled_components == i] = instance_id_counter\n",
    "            instance_id_counter += 1\n",
    "print(f\"合計 {instance_id_counter - 1} 個のプラスチック片（インスタンス）を検出しました。\")\n",
    "\n",
    "# --- ステップ2: スペクトルデータとラベルの結合 ---\n",
    "# (この部分は以前のスクリプトとほぼ同じ)\n",
    "wavelength_pattern = re.compile(r'Ex(\\d+)_Em(\\d+)')\n",
    "image_files = list(main_dir.glob(\"*.tiff\"))\n",
    "pixel_features_df = pd.DataFrame()\n",
    "\n",
    "for image_path in image_files:\n",
    "    if '-1_Em-1' in image_path.stem: continue\n",
    "    match = wavelength_pattern.search(image_path.name)\n",
    "    if not match: continue\n",
    "    ex, em = int(match.group(1)), int(match.group(2))\n",
    "    if ex == em: continue\n",
    "    try:\n",
    "        img = np.asarray(Image.open(image_path))\n",
    "        if img.shape[:2] != image_size: continue\n",
    "        pixel_features_df[f'Ex{ex}_Em{em}'] = img.flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to process {image_path.name}. Reason: {e}\")\n",
    "\n",
    "# --- ステップ3: 新しい列を追加して整形 ---\n",
    "print(\"\\nスペクトルデータにラベルとIDを追加します...\")\n",
    "pixel_features_df.reset_index(inplace=True)\n",
    "pixel_features_df.rename(columns={'index': 'original_index'}, inplace=True)\n",
    "\n",
    "# ラベル名をマッピング\n",
    "value_to_label_name = {v: k for k, v in label_name_to_value.items()}\n",
    "value_to_label_name[0] = '_unlabeled_' # ラベルなし領域\n",
    "pixel_features_df['label_name'] = pd.Series(class_label_mask.flatten()).map(value_to_label_name)\n",
    "\n",
    "# ★★★ インスタンスID列を追加 ★★★\n",
    "pixel_features_df['instance_id'] = instance_mask.flatten()\n",
    "\n",
    "# 不要なピクセルを除外（プラスチックのみを残す）\n",
    "labels_to_exclude = ['other', 'background', 'background_ref', '_unlabeled_']\n",
    "plastics_only_df = pixel_features_df[~pixel_features_df['label_name'].isin(labels_to_exclude)].copy()\n",
    "print(f\"プラスチックのピクセルのみを抽出しました。総ピクセル数: {len(plastics_only_df)}\")\n",
    "\n",
    "# --- ステップ4: CSVとして保存 ---\n",
    "output_dir = main_dir / \"csv\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "# ★★★ 新しいファイル名で保存 ★★★\n",
    "output_csv_path = output_dir / \"pixel_features_with_instance_id.csv\"\n",
    "plastics_only_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f'\\nインスタンスID付きの新しいデータセットが作成されました: {output_csv_path}')\n",
    "print(plastics_only_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# --- ユーザー設定 ---\n",
    "main_dir = Path(\"C:/Users/sawamoto24/sawamoto24/master/microplastic/data\")\n",
    "dataset1_folder_name = \"MPs_20250911\"\n",
    "dataset2_folder_name = \"MPs_20250905_2\"\n",
    "csv_filename = \"pixel_features_with_instance_id.csv\"\n",
    "# --------------------\n",
    "\n",
    "# --- ステップ1: 2つのデータセットを読み込み、結合 ---\n",
    "print(\"--- ステップ1: インスタンスID付きデータセットの結合 ---\")\n",
    "try:\n",
    "    df1 = pd.read_csv(main_dir / dataset1_folder_name / \"csv\" / csv_filename)\n",
    "    df2 = pd.read_csv(main_dir / dataset2_folder_name / \"csv\" / csv_filename)\n",
    "    \n",
    "    # instance_idが重複しないように、データセット2のIDにオフセットを追加\n",
    "    df2['instance_id'] = df2['instance_id'] + 1000 \n",
    "    \n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    print(\"データセットの結合が完了しました。\")\n",
    "    print(f\"総インスタンス数: {combined_df['instance_id'].nunique()} 個\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"エラー: CSVファイルが見つかりません: {e.filename}\")\n",
    "    print(\"両方のデータセットで「インスタンスID付きデータセットの作成」スクリプトを実行したか確認してください。\")\n",
    "    exit()\n",
    "\n",
    "# --- ステップ2: データ準備 ---\n",
    "print(\"\\n--- ステップ2: データ準備 ---\")\n",
    "X = combined_df.drop(columns=['label_name', 'original_index', 'instance_id'])\n",
    "y = combined_df['label_name']\n",
    "groups = combined_df['instance_id']\n",
    "\n",
    "# --- ステップ3: リーブ・ワン・アウト交差検証 (インスタンス単位) ---\n",
    "print(\"\\n--- ステップ3: インスタンス単位のリーブ・ワン・アウト交差検証を開始 ---\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "all_y_pred = []\n",
    "all_y_true = []\n",
    "num_splits = logo.get_n_splits(groups=groups)\n",
    "current_split = 0\n",
    "\n",
    "for train_idx, test_idx in logo.split(X, y, groups):\n",
    "    current_split += 1\n",
    "    print(f\"検証中... {current_split}/{num_splits}\")\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    all_y_pred.extend(y_pred)\n",
    "    all_y_true.extend(y_test)\n",
    "\n",
    "# --- ステップ4: 最終的な精度レポートの作成 ---\n",
    "print(\"\\n--- ステップ4: 最終的な精度レポート ---\")\n",
    "report_str = classification_report(all_y_true, all_y_pred)\n",
    "print(\"インスタンス単位での交差検証が完了しました。\")\n",
    "print(report_str)\n",
    "\n",
    "# --- ステップ5: 予測結果をファイルに保存 ---\n",
    "results_df = pd.DataFrame({\n",
    "    'true_label': all_y_true,\n",
    "    'predicted_label': all_y_pred\n",
    "})\n",
    "output_dir = Path(\"C:/Users/sawamoto24/sawamoto24/master/microplastic/results/インスタンス単位検証\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_csv_path = output_dir / \"prediction_results_loocv.csv\"\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "report_output_path = output_dir / \"classification_report_instance_level_loocv.txt\"\n",
    "with open(report_output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"--- インスタンス単位 リーブ・ワン・アウト交差検証 精度レポート ---\\n\\n\")\n",
    "    f.write(report_str)\n",
    "\n",
    "print(f\"\\n予測結果とレポートを {output_dir} に保存しました。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- ユーザー設定 ---\n",
    "# 検証結果が保存されているCSVファイルのパス\n",
    "results_dir = Path(\"C:/Users/sawamoto24/sawamoto24/master/microplastic/results/インスタンス単位検証\")\n",
    "results_csv_path = results_dir / \"prediction_results_loocv.csv\"\n",
    "# --------------------\n",
    "\n",
    "# 1. 予測結果のCSVファイルを読み込む\n",
    "print(f\"予測結果ファイルを読み込みます: {results_csv_path}\")\n",
    "try:\n",
    "    results_df = pd.read_csv(results_csv_path)\n",
    "    y_true = results_df['true_label']\n",
    "    y_pred = results_df['predicted_label']\n",
    "except FileNotFoundError:\n",
    "    print(f\"エラー: 予測結果ファイルが見つかりません。\")\n",
    "    print(\"先に「インスタンス単位でのリーブ・ワン・アウト交差検証」スクリプトを実行してください。\")\n",
    "    exit()\n",
    "\n",
    "# 2. 混同行列の計算\n",
    "print(\"\\n--- 混同行列の計算 ---\")\n",
    "labels = sorted(y_true.unique())\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "# 3. 混同行列の可視化 (生データ)\n",
    "print(\"\\n--- 混同行列の可視化 (絶対数) ---\")\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix (Instance-Level LOOCV - Raw Counts)', fontsize=16)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# 4. 正規化された混同行列の可視化 (割合)\n",
    "print(\"\\n--- 正規化された混同行列の可視化 (Recall) ---\")\n",
    "cm_sum = cm.sum(axis=1)[:, np.newaxis]\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    cm_normalized = np.nan_to_num(cm.astype('float') / cm_sum)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Normalized Confusion Matrix (Instance-Level LOOCV - Recall)', fontsize=16)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64d617",
   "metadata": {},
   "source": [
    "### クラス別分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f266f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- ユーザー設定 ---\n",
    "main_dir = Path(\"C:/Users/sawamoto24/sawamoto24/master/microplastic/data\")\n",
    "dataset1_folder_name = \"MPs_20250911\"\n",
    "dataset2_folder_name = \"MPs_20250905_2\"\n",
    "csv_filename = \"pixel_features_with_instance_id.csv\"\n",
    "\n",
    "# デンドログラムに基づいた階層（サブクラスタ）を定義\n",
    "level1_map = {\n",
    "    'PVC': 'Group_A', 'PS': 'Group_A', 'ABS': 'Group_A', 'PET': 'Group_A',\n",
    "    'PMMA': 'Group_B', 'PC': 'Group_B', 'PP': 'Group_B', 'LDPE': 'Group_B', 'HDPE': 'Group_B'\n",
    "}\n",
    "level2_map = {\n",
    "    'PVC': 'Subgroup_A1 (PVC/PS)', 'PS': 'Subgroup_A1 (PVC/PS)', \n",
    "    'ABS': 'Subgroup_A2 (ABS)', \n",
    "    'PET': 'Subgroup_A3 (PET)'\n",
    "}\n",
    "level3_map = {\n",
    "    'PP': 'Subgroup_B1 (PP/LDPE)', 'LDPE': 'Subgroup_B1 (PP/LDPE)', \n",
    "    'HDPE': 'Subgroup_B2 (HDPE)', \n",
    "    'PC': 'Subgroup_B3 (PC/PMMA)', 'PMMA': 'Subgroup_B3 (PC/PMMA)'\n",
    "}\n",
    "# --------------------\n",
    "\n",
    "# 1. データの読み込みと結合\n",
    "print(\"--- ステップ1: 階層ラベルデータセットの作成 ---\")\n",
    "try:\n",
    "    df1 = pd.read_csv(main_dir / dataset1_folder_name / \"csv\" / csv_filename)\n",
    "    df2 = pd.read_csv(main_dir / dataset2_folder_name / \"csv\" / csv_filename)\n",
    "    \n",
    "    # instance_idが重複しないようにオフセットを追加\n",
    "    df2['instance_id'] = df2['instance_id'] + 1000 \n",
    "    \n",
    "    combined_df_hierarchical = pd.concat([df1, df2], ignore_index=True)\n",
    "    print(\"データセットの結合が完了しました。\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"エラー: CSVファイルが見つかりません: {e.filename}\")\n",
    "    print(\"両方のデータセットで「インスタンスID付きデータセットの作成」スクリプトを実行したか確認してください。\")\n",
    "    # exit() # ipynbではexit()をコメントアウトした方が安全です\n",
    "\n",
    "# 2. 新しい階層ラベル列を追加\n",
    "combined_df_hierarchical['level1'] = combined_df_hierarchical['label_name'].map(level1_map)\n",
    "combined_df_hierarchical['level2'] = combined_df_hierarchical['label_name'].map(level2_map)\n",
    "combined_df_hierarchical['level3'] = combined_df_hierarchical['label_name'].map(level3_map)\n",
    "\n",
    "print(\"階層ラベルの追加が完了しました。\")\n",
    "print(\"データセットのプレビュー:\")\n",
    "print(combined_df_hierarchical[['label_name', 'level1', 'level2', 'level3']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from pathlib import Path\n",
    "\n",
    "# --- ユーザー設定 ---\n",
    "# 評価したい階層レベルをリストで指定\n",
    "target_levels = ['level1', 'level2', 'level3', 'label_name'] \n",
    "# 結果を保存するフォルダを定義\n",
    "output_dir = Path(\"C:/Users/sawamoto24/sawamoto24/master/microplastic/results/階層別分類_インスタンス単位検証\")\n",
    "# --------------------\n",
    "\n",
    "# 出力フォルダの作成\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"結果は {output_dir} に保存されます。\")\n",
    "\n",
    "# --- ループ処理で各レベルを評価 ---\n",
    "for target_level in target_levels:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"--- {target_level} レベルでのインスタンス単位LOOCVを開始 ---\")\n",
    "\n",
    "    # 1. データ準備\n",
    "    # NaNが含まれる行（その階層に属さないクラス）を除外\n",
    "    level_df = combined_df_hierarchical.dropna(subset=[target_level])\n",
    "\n",
    "    if level_df.empty:\n",
    "        print(f\"{target_level} に該当するデータがありません。スキップします。\")\n",
    "        continue\n",
    "\n",
    "    # 特徴量 (X), ラベル (y), グループ (instance_id) を準備\n",
    "    X = level_df.drop(columns=['label_name', 'original_index', 'instance_id', 'level1', 'level2', 'level3'])\n",
    "    y = level_df[target_level]\n",
    "    groups = level_df['instance_id']\n",
    "\n",
    "    # データが1クラスしかない場合は分割できないためスキップ\n",
    "    if y.nunique() < 2:\n",
    "        print(f\"{target_level} のクラスが1種類しかないため、評価をスキップします。\")\n",
    "        continue\n",
    "\n",
    "    # 2. インスタンス単位のリーブ・ワン・アウト交差検証\n",
    "    logo = LeaveOneGroupOut()\n",
    "    all_y_pred = []\n",
    "    all_y_true = []\n",
    "    \n",
    "    num_splits = logo.get_n_splits(groups=groups)\n",
    "    current_split = 0\n",
    "\n",
    "    for train_idx, test_idx in logo.split(X, y, groups):\n",
    "        current_split += 1\n",
    "        print(f\"検証中... {current_split}/{num_splits}\")\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        all_y_pred.extend(y_pred)\n",
    "        all_y_true.extend(y_test)\n",
    "\n",
    "    # 3. 最終的な精度レポートの作成と保存\n",
    "    report_str = classification_report(all_y_true, all_y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"\\n--- {target_level} 精度評価レポート ---\")\n",
    "    print(report_str)\n",
    "\n",
    "    output_path = output_dir / f\"classification_report_{target_level}_instance_level.txt\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"--- {target_level} | インスタンス単位LOOCV 精度レポート ---\\n\\n\")\n",
    "        f.write(report_str)\n",
    "    print(f\"レポートを {output_path} に保存しました。\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"全ての階層レベルの評価が完了しました。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd706dc",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
